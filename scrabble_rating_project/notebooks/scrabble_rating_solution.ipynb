{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scrabble Player Rating Prediction\n",
        "\n",
        "## Практическое задание курса Light Auto ML\n",
        "\n",
        "**Соревнование:** [Scrabble Player Rating](https://www.kaggle.com/competitions/scrabble-player-rating/data)\n",
        "\n",
        "**Задача:** Предсказать рейтинг игрока до игры на основе метаданных игры и данных о ходах\n",
        "\n",
        "**Тип задачи:** Регрессия\n",
        "\n",
        "---\n",
        "\n",
        "## Содержание\n",
        "\n",
        "1. [Импорт библиотек и настройка](#1)\n",
        "2. [Загрузка данных](#2)\n",
        "3. [Анализ целевой переменной](#3)\n",
        "4. [Анализ признаков](#4)\n",
        "5. [Бейзлайн с LightAutoML](#5)\n",
        "6. [Собственное решение](#6)\n",
        "7. [Выводы и сравнение результатов](#7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='1'></a>\n",
        "## 1. Импорт библиотек и настройка\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Standard libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# LightAutoML\n",
        "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
        "from lightautoml.tasks import Task\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants\n",
        "RANDOM_STATE = 42\n",
        "N_THREADS = 4\n",
        "N_FOLDS = 5\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# Paths - определяем корневую директорию проекта\n",
        "# Если ноутбук запущен из папки notebooks/, то PROJECT_ROOT будет родительской директорией\n",
        "current_dir = Path.cwd()\n",
        "if current_dir.name == 'notebooks':\n",
        "    PROJECT_ROOT = current_dir.parent\n",
        "else:\n",
        "    # Если запущен из корня проекта или другой директории, ищем папку scrabble_rating_project\n",
        "    if 'scrabble_rating_project' in str(current_dir):\n",
        "        # Находим корень проекта\n",
        "        parts = current_dir.parts\n",
        "        idx = list(parts).index('scrabble_rating_project') if 'scrabble_rating_project' in parts else -1\n",
        "        if idx >= 0:\n",
        "            PROJECT_ROOT = Path(*parts[:idx+1])\n",
        "        else:\n",
        "            PROJECT_ROOT = current_dir\n",
        "    else:\n",
        "        PROJECT_ROOT = current_dir\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Current working directory: {Path.cwd()}\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Results directory: {RESULTS_DIR}\")\n",
        "\n",
        "# Проверка наличия данных\n",
        "data_files = ['games.csv', 'turns.csv', 'train.csv', 'test.csv', 'sample_submission.csv']\n",
        "print(f\"\\nПроверка наличия файлов данных:\")\n",
        "all_files_exist = True\n",
        "for file in data_files:\n",
        "    file_path = DATA_DIR / file\n",
        "    exists = file_path.exists()\n",
        "    if exists:\n",
        "        size = file_path.stat().st_size / (1024*1024)\n",
        "        print(f\"  ✓ {file}: {size:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"  ✗ {file}: не найден\")\n",
        "        all_files_exist = False\n",
        "\n",
        "if not all_files_exist:\n",
        "    print(f\"\\n⚠️  ВНИМАНИЕ: Не все файлы данных найдены!\")\n",
        "    print(f\"   Убедитесь, что данные находятся в папке: {DATA_DIR}\")\n",
        "else:\n",
        "    print(f\"\\n✓ Все файлы данных найдены!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='2'></a>\n",
        "## 2. Загрузка данных\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. Обоснование стратегии разделения данных\n",
        "\n",
        "**Важно:** Для предотвращения утечки данных (data leakage) необходимо правильно разделить данные.\n",
        "\n",
        "**Особенности данных:**\n",
        "- Данные содержат информацию о разных игроках (nickname)\n",
        "- Один и тот же игрок может встречаться в нескольких играх\n",
        "- Рейтинги игроков могут быть связаны между играми\n",
        "\n",
        "**Стратегия разделения:**\n",
        "- Используем случайное разделение с фиксированным random_state для воспроизводимости\n",
        "- Разделение происходит на уровне записей (игр), а не игроков\n",
        "- Это допустимо, так как задача - предсказать рейтинг на основе метаданных игры и ходов\n",
        "- Для более строгого подхода можно было бы использовать групповое разделение по игрокам, но это не требуется для данной задачи\n",
        "\n",
        "**Предотвращение утечки данных:**\n",
        "- Все признаки агрегируются ДО разделения на train/validation\n",
        "- Не используем информацию из будущего (например, финальный счет для предсказания рейтинга до игры)\n",
        "- Рейтинги в train.csv - это рейтинги ДО игры, что соответствует задаче\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "games_df = pd.read_csv(DATA_DIR / 'games.csv')\n",
        "turns_df = pd.read_csv(DATA_DIR / 'turns.csv')\n",
        "train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
        "test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
        "sample_submission_df = pd.read_csv(DATA_DIR / 'sample_submission.csv')\n",
        "\n",
        "print(\"Data shapes:\")\n",
        "print(f\"games_df: {games_df.shape}\")\n",
        "print(f\"turns_df: {turns_df.shape}\")\n",
        "print(f\"train_df: {train_df.shape}\")\n",
        "print(f\"test_df: {test_df.shape}\")\n",
        "print(f\"sample_submission_df: {sample_submission_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic info about datasets\n",
        "print(\"\\n=== Games DataFrame Info ===\")\n",
        "print(games_df.info())\n",
        "print(\"\\n=== Games DataFrame Head ===\")\n",
        "print(games_df.head())\n",
        "\n",
        "print(\"\\n=== Turns DataFrame Info ===\")\n",
        "print(turns_df.info())\n",
        "print(\"\\n=== Turns DataFrame Head ===\")\n",
        "print(turns_df.head())\n",
        "\n",
        "print(\"\\n=== Train DataFrame Info ===\")\n",
        "print(train_df.info())\n",
        "print(\"\\n=== Train DataFrame Head ===\")\n",
        "print(train_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='3'></a>\n",
        "## 3. Анализ целевой переменной\n",
        "\n",
        "Целевая переменная: `rating` - рейтинг игрока до игры\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "target_stats = train_df['rating'].describe()\n",
        "print(\"=== Статистика целевой переменной (rating) ===\")\n",
        "print(target_stats)\n",
        "print(f\"\\nКоличество уникальных значений: {train_df['rating'].nunique()}\")\n",
        "print(f\"Пропущенные значения: {train_df['rating'].isna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Histogram\n",
        "axes[0, 0].hist(train_df['rating'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title('Распределение рейтинга (гистограмма)', fontsize=14)\n",
        "axes[0, 0].set_xlabel('Рейтинг')\n",
        "axes[0, 0].set_ylabel('Частота')\n",
        "axes[0, 0].axvline(train_df['rating'].mean(), color='r', linestyle='--', label=f'Среднее: {train_df[\"rating\"].mean():.2f}')\n",
        "axes[0, 0].axvline(train_df['rating'].median(), color='g', linestyle='--', label=f'Медиана: {train_df[\"rating\"].median():.2f}')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Box plot\n",
        "axes[0, 1].boxplot(train_df['rating'], vert=True)\n",
        "axes[0, 1].set_title('Распределение рейтинга (ящик с усами)', fontsize=14)\n",
        "axes[0, 1].set_ylabel('Рейтинг')\n",
        "\n",
        "# Q-Q plot\n",
        "from scipy import stats\n",
        "stats.probplot(train_df['rating'], dist=\"norm\", plot=axes[1, 0])\n",
        "axes[1, 0].set_title('Q-Q Plot (нормальность распределения)', fontsize=14)\n",
        "\n",
        "# Density plot\n",
        "train_df['rating'].plot(kind='density', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Плотность распределения рейтинга', fontsize=14)\n",
        "axes[1, 1].set_xlabel('Рейтинг')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'target_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell was moved - feature_types classification now happens in the outlier analysis cell (cell 17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier detection using IQR method\n",
        "Q1 = train_df['rating'].quantile(0.25)\n",
        "Q3 = train_df['rating'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = train_df[(train_df['rating'] < lower_bound) | (train_df['rating'] > upper_bound)]\n",
        "print(f\"=== Анализ аномальных значений ===\")\n",
        "print(f\"Q1: {Q1:.2f}\")\n",
        "print(f\"Q3: {Q3:.2f}\")\n",
        "print(f\"IQR: {IQR:.2f}\")\n",
        "print(f\"Нижняя граница: {lower_bound:.2f}\")\n",
        "print(f\"Верхняя граница: {upper_bound:.2f}\")\n",
        "print(f\"Количество аномальных значений: {len(outliers)} ({len(outliers)/len(train_df)*100:.2f}%)\")\n",
        "\n",
        "if len(outliers) > 0:\n",
        "    print(f\"\\nМинимальное значение: {train_df['rating'].min():.2f}\")\n",
        "    print(f\"Максимальное значение: {train_df['rating'].max():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporal analysis if 'created_at' exists in games_df\n",
        "if 'created_at' in games_df.columns:\n",
        "    # Merge to get ratings over time\n",
        "    train_with_time = train_df.merge(games_df[['game_id', 'created_at']], on='game_id', how='left')\n",
        "    train_with_time['created_at'] = pd.to_datetime(train_with_time['created_at'])\n",
        "    train_with_time = train_with_time.sort_values('created_at')\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "    \n",
        "    # Rating over time\n",
        "    axes[0].scatter(train_with_time['created_at'], train_with_time['rating'], alpha=0.1, s=1)\n",
        "    axes[0].set_title('Рейтинг во времени', fontsize=14)\n",
        "    axes[0].set_xlabel('Дата')\n",
        "    axes[0].set_ylabel('Рейтинг')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Rolling mean\n",
        "    train_with_time['rating_rolling_mean'] = train_with_time['rating'].rolling(window=1000).mean()\n",
        "    axes[1].plot(train_with_time['created_at'], train_with_time['rating_rolling_mean'], color='red', linewidth=2)\n",
        "    axes[1].set_title('Скользящее среднее рейтинга (окно 1000)', fontsize=14)\n",
        "    axes[1].set_xlabel('Дата')\n",
        "    axes[1].set_ylabel('Рейтинг (скользящее среднее)')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(RESULTS_DIR / 'target_temporal.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='4'></a>\n",
        "## 4. Анализ признаков\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import utility functions\n",
        "import sys\n",
        "sys.path.append(str(PROJECT_ROOT / 'src'))\n",
        "from utils.feature_engineering import aggregate_turns_features, classify_features, create_additional_features\n",
        "\n",
        "# Create aggregated features\n",
        "turns_features = aggregate_turns_features(turns_df)\n",
        "print(f\"Turns features shape: {turns_features.shape}\")\n",
        "print(turns_features.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge all data\n",
        "train_merged = (\n",
        "    train_df\n",
        "    .merge(games_df, on='game_id', how='left')\n",
        "    .merge(turns_features, on=['game_id', 'nickname'], how='left')\n",
        ")\n",
        "\n",
        "print(f\"Merged train shape: {train_merged.shape}\")\n",
        "print(f\"\\nColumns: {list(train_merged.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2. Выявление аномальных значений в признаках\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Анализ аномальных значений в числовых признаках\n",
        "print(\"=== Анализ аномальных значений в признаках ===\")\n",
        "\n",
        "# Classify features if not already done\n",
        "if 'feature_types' not in globals() or feature_types is None:\n",
        "    feature_types = classify_features(train_merged)\n",
        "\n",
        "numeric_cols_for_outliers = feature_types['numeric'][:15]  # Первые 15 числовых признаков\n",
        "\n",
        "outlier_summary = []\n",
        "for col in numeric_cols_for_outliers:\n",
        "    if col in train_merged.columns:\n",
        "        Q1 = train_merged[col].quantile(0.25)\n",
        "        Q3 = train_merged[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        \n",
        "        if IQR > 0:  # Избегаем деления на ноль\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers_count = ((train_merged[col] < lower_bound) | (train_merged[col] > upper_bound)).sum()\n",
        "            outlier_percent = (outliers_count / len(train_merged)) * 100\n",
        "            \n",
        "            if outliers_count > 0:\n",
        "                outlier_summary.append({\n",
        "                    'Feature': col,\n",
        "                    'Outliers': outliers_count,\n",
        "                    'Percent': outlier_percent,\n",
        "                    'Min': train_merged[col].min(),\n",
        "                    'Max': train_merged[col].max(),\n",
        "                    'Lower_bound': lower_bound,\n",
        "                    'Upper_bound': upper_bound\n",
        "                })\n",
        "\n",
        "if outlier_summary:\n",
        "    outlier_df = pd.DataFrame(outlier_summary)\n",
        "    print(\"\\nПризнаки с аномальными значениями:\")\n",
        "    print(outlier_df.sort_values('Percent', ascending=False))\n",
        "    \n",
        "    # Визуализация топ-5 признаков с наибольшим процентом аномалий\n",
        "    top_outliers = outlier_df.nlargest(5, 'Percent')\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Процент аномалий\n",
        "    axes[0].barh(top_outliers['Feature'], top_outliers['Percent'])\n",
        "    axes[0].set_title('Топ-5 признаков по проценту аномальных значений', fontsize=14)\n",
        "    axes[0].set_xlabel('Процент аномальных значений')\n",
        "    \n",
        "    # Количество аномалий\n",
        "    axes[1].barh(top_outliers['Feature'], top_outliers['Outliers'])\n",
        "    axes[1].set_title('Топ-5 признаков по количеству аномальных значений', fontsize=14)\n",
        "    axes[1].set_xlabel('Количество аномальных значений')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(RESULTS_DIR / 'feature_outliers.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Аномальные значения не обнаружены в анализируемых признаках\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature type classification\n",
        "feature_types = classify_features(train_merged)\n",
        "print(\"=== Классификация признаков ===\")\n",
        "print(f\"Числовые признаки ({len(feature_types['numeric'])}): {feature_types['numeric']}\")\n",
        "print(f\"\\nКатегориальные признаки ({len(feature_types['categorical'])}): {feature_types['categorical']}\")\n",
        "print(f\"\\nВременные признаки ({len(feature_types['datetime'])}): {feature_types['datetime']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values analysis\n",
        "missing_values = train_merged.isnull().sum()\n",
        "missing_percent = (missing_values / len(train_merged)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_values,\n",
        "    'Missing Percent': missing_percent\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "\n",
        "print(\"=== Анализ пропущенных значений ===\")\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "# Visualization\n",
        "if missing_df['Missing Count'].sum() > 0:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    missing_df[missing_df['Missing Count'] > 0]['Missing Percent'].plot(kind='barh')\n",
        "    plt.title('Процент пропущенных значений по признакам', fontsize=14)\n",
        "    plt.xlabel('Процент пропущенных значений')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(RESULTS_DIR / 'missing_values.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature distributions visualization\n",
        "numeric_cols = feature_types['numeric'][:10]  # First 10 numeric features\n",
        "\n",
        "if len(numeric_cols) > 0:\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, col in enumerate(numeric_cols):\n",
        "        if idx < len(axes):\n",
        "            train_merged[col].hist(bins=30, ax=axes[idx], edgecolor='black', alpha=0.7)\n",
        "            axes[idx].set_title(f'{col}', fontsize=10)\n",
        "            axes[idx].set_xlabel('')\n",
        "            axes[idx].set_ylabel('')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(RESULTS_DIR / 'numeric_features_distributions.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "numeric_cols_all = feature_types['numeric']\n",
        "correlation_matrix = train_merged[numeric_cols_all + ['rating']].corr()\n",
        "\n",
        "# Correlation with target\n",
        "target_corr = correlation_matrix['rating'].sort_values(ascending=False)\n",
        "print(\"=== Корреляция признаков с целевой переменной ===\")\n",
        "print(target_corr)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Матрица корреляций', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance visualization (top correlations)\n",
        "top_corr = target_corr.drop('rating').abs().sort_values(ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_corr.plot(kind='barh')\n",
        "plt.title('Топ-15 признаков по абсолютной корреляции с целевой переменной', fontsize=14)\n",
        "plt.xlabel('Абсолютная корреляция')\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'feature_importance_correlation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical features analysis\n",
        "categorical_cols = feature_types['categorical']\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    fig, axes = plt.subplots(len(categorical_cols), 1, figsize=(12, 4 * len(categorical_cols)))\n",
        "    if len(categorical_cols) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, col in enumerate(categorical_cols):\n",
        "        value_counts = train_merged[col].value_counts().head(10)\n",
        "        value_counts.plot(kind='bar', ax=axes[idx])\n",
        "        axes[idx].set_title(f'Распределение значений: {col}', fontsize=12)\n",
        "        axes[idx].set_xlabel('')\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(RESULTS_DIR / 'categorical_features_distributions.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='5'></a>\n",
        "## 5. Бейзлайн с LightAutoML\n",
        "\n",
        "Создадим минимум 2 различные конфигурации и выберем лучшую.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for LightAutoML\n",
        "# Remove target and ID columns\n",
        "feature_cols = [col for col in train_merged.columns \n",
        "                if col not in ['rating', 'game_id', 'nickname']]\n",
        "\n",
        "X_train = train_merged[feature_cols].copy()\n",
        "y_train = train_merged['rating'].copy()\n",
        "\n",
        "# Handle missing values\n",
        "X_train = X_train.fillna(0)\n",
        "\n",
        "# Split for validation\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train, y_train, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"Train shape: {X_train_split.shape}\")\n",
        "print(f\"Validation shape: {X_val_split.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration 1: Default TabularAutoML\n",
        "print(\"=== Конфигурация 1: TabularAutoML (default) ===\")\n",
        "\n",
        "task1 = Task('reg', metric='mse')\n",
        "automl1 = TabularAutoML(\n",
        "    task=task1,\n",
        "    timeout=240,  # 4 minutes\n",
        "    cpu_limit=N_THREADS,\n",
        "    general_params={'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]}\n",
        ")\n",
        "\n",
        "train_data_1 = pd.concat([X_train_split, y_train_split], axis=1)\n",
        "train_data_1.columns = list(X_train_split.columns) + ['rating']\n",
        "\n",
        "start_time = time.time()\n",
        "oof_pred1 = automl1.fit_predict(\n",
        "    train_data_1,\n",
        "    roles={'target': 'rating'},\n",
        "    verbose=1\n",
        ")\n",
        "time1 = time.time() - start_time\n",
        "\n",
        "val_pred1 = automl1.predict(X_val_split)\n",
        "rmse1 = np.sqrt(mean_squared_error(y_val_split, val_pred1.data[:, 0]))\n",
        "mae1 = mean_absolute_error(y_val_split, val_pred1.data[:, 0])\n",
        "r2_1 = r2_score(y_val_split, val_pred1.data[:, 0])\n",
        "\n",
        "print(f\"\\nВремя обучения: {time1:.2f} секунд\")\n",
        "print(f\"RMSE: {rmse1:.4f}\")\n",
        "print(f\"MAE: {mae1:.4f}\")\n",
        "print(f\"R2 Score: {r2_1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration 2: TabularAutoML with more algorithms\n",
        "print(\"=== Конфигурация 2: TabularAutoML (extended algorithms) ===\")\n",
        "\n",
        "task2 = Task('reg', metric='mse')\n",
        "automl2 = TabularAutoML(\n",
        "    task=task2,\n",
        "    timeout=360,  # 6 minutes\n",
        "    cpu_limit=N_THREADS,\n",
        "    general_params={'use_algos': [['linear_l2', 'lgb', 'lgb_tuned', 'cb', 'cb_tuned']]}\n",
        ")\n",
        "\n",
        "train_data_2 = pd.concat([X_train_split, y_train_split], axis=1)\n",
        "train_data_2.columns = list(X_train_split.columns) + ['rating']\n",
        "\n",
        "start_time = time.time()\n",
        "oof_pred2 = automl2.fit_predict(\n",
        "    train_data_2,\n",
        "    roles={'target': 'rating'},\n",
        "    verbose=1\n",
        ")\n",
        "time2 = time.time() - start_time\n",
        "\n",
        "val_pred2 = automl2.predict(X_val_split)\n",
        "rmse2 = np.sqrt(mean_squared_error(y_val_split, val_pred2.data[:, 0]))\n",
        "mae2 = mean_absolute_error(y_val_split, val_pred2.data[:, 0])\n",
        "r2_2 = r2_score(y_val_split, val_pred2.data[:, 0])\n",
        "\n",
        "print(f\"\\nВремя обучения: {time2:.2f} секунд\")\n",
        "print(f\"RMSE: {rmse2:.4f}\")\n",
        "print(f\"MAE: {mae2:.4f}\")\n",
        "print(f\"R2 Score: {r2_2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare configurations\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Configuration': ['Config 1 (LGB)', 'Config 2 (LGB + CB)'],\n",
        "    'RMSE': [rmse1, rmse2],\n",
        "    'MAE': [mae1, mae2],\n",
        "    'R2 Score': [r2_1, r2_2],\n",
        "    'Time (seconds)': [time1, time2]\n",
        "})\n",
        "\n",
        "print(\"=== Сравнение конфигураций LightAutoML ===\")\n",
        "print(comparison_df)\n",
        "\n",
        "# Select best model\n",
        "best_config_idx = comparison_df['RMSE'].idxmin()\n",
        "best_automl = automl1 if best_config_idx == 0 else automl2\n",
        "best_rmse = comparison_df.loc[best_config_idx, 'RMSE']\n",
        "\n",
        "print(f\"\\nЛучшая конфигурация: {comparison_df.loc[best_config_idx, 'Configuration']}\")\n",
        "print(f\"Лучший RMSE: {best_rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='6'></a>\n",
        "## 6. Собственное решение\n",
        "\n",
        "Создадим собственный пайплайн без использования LightAutoML, демонстрируя принципы AutoML:\n",
        "\n",
        "1. **Автоматический выбор признаков** - автоматический отбор наиболее информативных признаков\n",
        "2. **Автоматическая предобработка** - обработка пропусков, кодирование категориальных переменных\n",
        "3. **Автоматический выбор модели** - сравнение нескольких алгоритмов и выбор лучшего\n",
        "4. **Автоматическая оптимизация гиперпараметров** - поиск оптимальных параметров модели\n",
        "5. **Автоматическое ансамблирование** - комбинирование нескольких моделей с оптимизацией весов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create enhanced features\n",
        "train_enhanced = create_additional_features(train_df, games_df, turns_features)\n",
        "print(f\"Enhanced train shape: {train_enhanced.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for custom pipeline\n",
        "feature_cols_enhanced = [col for col in train_enhanced.columns \n",
        "                        if col not in ['rating', 'game_id', 'nickname']]\n",
        "\n",
        "X_train_custom = train_enhanced[feature_cols_enhanced].copy()\n",
        "y_train_custom = train_enhanced['rating'].copy()\n",
        "\n",
        "# Split data\n",
        "X_train_custom_split, X_val_custom_split, y_train_custom_split, y_val_custom_split = train_test_split(\n",
        "    X_train_custom, y_train_custom, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"Custom train shape: {X_train_custom_split.shape}\")\n",
        "print(f\"Custom validation shape: {X_val_custom_split.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline 1: LightGBM with feature selection\n",
        "# AutoML принцип: Автоматический выбор признаков (Feature Selection)\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "import numpy as np\n",
        "\n",
        "print(\"=== Pipeline 1: LightGBM с автоматическим отбором признаков ===\")\n",
        "print(\"Принцип AutoML: Автоматический выбор наиболее информативных признаков\")\n",
        "\n",
        "# Обработка категориальных признаков перед feature selection\n",
        "categorical_cols_p1 = [col for col in X_train_custom_split.columns \n",
        "                       if X_train_custom_split[col].dtype == 'object']\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "X_train_encoded_p1 = X_train_custom_split.copy()\n",
        "X_val_encoded_p1 = X_val_custom_split.copy()\n",
        "\n",
        "label_encoders_p1 = {}\n",
        "for col in categorical_cols_p1:\n",
        "    le = LabelEncoder()\n",
        "    # train\n",
        "    train_col = X_train_encoded_p1[col].astype(str).fillna('unknown')\n",
        "    le.fit(train_col)\n",
        "    # гарантируем наличие 'unknown' в классах\n",
        "    if 'unknown' not in le.classes_:\n",
        "        le.classes_ = np.append(le.classes_, 'unknown')\n",
        "    X_train_encoded_p1[col] = le.transform(train_col)\n",
        "\n",
        "    # val: неизвестные значения мапим в 'unknown'\n",
        "    val_col = X_val_encoded_p1[col].astype(str).fillna('unknown')\n",
        "    known = set(le.classes_)\n",
        "    val_col = val_col.apply(lambda x: x if x in known else 'unknown')\n",
        "    X_val_encoded_p1[col] = le.transform(val_col)\n",
        "\n",
        "    label_encoders_p1[col] = le\n",
        "\n",
        "# Заполняем пропуски\n",
        "X_train_encoded_p1 = X_train_encoded_p1.fillna(0)\n",
        "X_val_encoded_p1 = X_val_encoded_p1.fillna(0)\n",
        "\n",
        "# Используем только числовые признаки (без дат и категориальных)\n",
        "X_train_num_p1 = X_train_encoded_p1.select_dtypes(include=[np.number])\n",
        "X_val_num_p1 = X_val_encoded_p1.select_dtypes(include=[np.number])\n",
        "\n",
        "# Автоматический выбор признаков - пробуем разные методы\n",
        "# Метод 1: F-статистика\n",
        "selector_f = SelectKBest(f_regression, k=50)\n",
        "X_train_f = selector_f.fit_transform(X_train_num_p1, y_train_custom_split)\n",
        "\n",
        "# Метод 2: Mutual Information\n",
        "try:\n",
        "    selector_mi = SelectKBest(mutual_info_regression, k=50)\n",
        "    X_train_mi = selector_mi.fit_transform(X_train_num_p1, y_train_custom_split)\n",
        "    \n",
        "    # Выбираем лучший метод на основе кросс-валидации\n",
        "    from sklearn.model_selection import cross_val_score\n",
        "    temp_model = LGBMRegressor(n_estimators=100, random_state=RANDOM_STATE, verbose=-1)\n",
        "    \n",
        "    score_f = cross_val_score(temp_model, X_train_f, y_train_custom_split, \n",
        "                              cv=3, scoring='neg_mean_squared_error').mean()\n",
        "    score_mi = cross_val_score(temp_model, X_train_mi, y_train_custom_split, \n",
        "                               cv=3, scoring='neg_mean_squared_error').mean()\n",
        "    \n",
        "    if score_mi > score_f:\n",
        "        selector = selector_mi\n",
        "        print(f\"Выбран метод: Mutual Information (score: {score_mi:.4f})\")\n",
        "    else:\n",
        "        selector = selector_f\n",
        "        print(f\"Выбран метод: F-regression (score: {score_f:.4f})\")\n",
        "except:\n",
        "    selector = selector_f\n",
        "    print(\"Используется метод: F-regression\")\n",
        "\n",
        "X_train_selected = selector.fit_transform(X_train_num_p1, y_train_custom_split)\n",
        "X_val_selected = selector.transform(X_val_num_p1)\n",
        "\n",
        "# Model\n",
        "model1 = LGBMRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    num_leaves=31,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=N_THREADS,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "model1.fit(X_train_selected, y_train_custom_split)\n",
        "time_p1 = time.time() - start_time\n",
        "\n",
        "val_pred_p1 = model1.predict(X_val_selected)\n",
        "rmse_p1 = np.sqrt(mean_squared_error(y_val_custom_split, val_pred_p1))\n",
        "mae_p1 = mean_absolute_error(y_val_custom_split, val_pred_p1)\n",
        "r2_p1 = r2_score(y_val_custom_split, val_pred_p1)\n",
        "\n",
        "print(f\"Время обучения: {time_p1:.2f} секунд\")\n",
        "print(f\"RMSE: {rmse_p1:.4f}\")\n",
        "print(f\"MAE: {mae_p1:.4f}\")\n",
        "print(f\"R2 Score: {r2_p1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline 2: CatBoost with preprocessing\n",
        "# AutoML принцип: Автоматическая предобработка данных\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "print(\"=== Pipeline 2: CatBoost с автоматической предобработкой ===\")\n",
        "print(\"Принцип AutoML: Автоматическая обработка категориальных признаков и пропусков\")\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols_custom = [col for col in X_train_custom_split.columns \n",
        "                           if X_train_custom_split[col].dtype == 'object']\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "X_train_encoded = X_train_custom_split.copy()\n",
        "X_val_encoded = X_val_custom_split.copy()\n",
        "\n",
        "for col in categorical_cols_custom:\n",
        "    le = LabelEncoder()\n",
        "    # train\n",
        "    train_col = X_train_encoded[col].astype(str).fillna('unknown')\n",
        "    le.fit(train_col)\n",
        "    # гарантируем наличие 'unknown' в классах\n",
        "    if 'unknown' not in le.classes_:\n",
        "        le.classes_ = np.append(le.classes_, 'unknown')\n",
        "    X_train_encoded[col] = le.transform(train_col)\n",
        "\n",
        "    # val: неизвестные значения мапим в 'unknown'\n",
        "    val_col = X_val_encoded[col].astype(str).fillna('unknown')\n",
        "    known = set(le.classes_)\n",
        "    val_col = val_col.apply(lambda x: x if x in known else 'unknown')\n",
        "    X_val_encoded[col] = le.transform(val_col)\n",
        "\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Fill missing values\n",
        "X_train_encoded = X_train_encoded.fillna(0)\n",
        "X_val_encoded = X_val_encoded.fillna(0)\n",
        "\n",
        "# Model\n",
        "model2 = CatBoostRegressor(\n",
        "    iterations=300,\n",
        "    learning_rate=0.05,\n",
        "    depth=7,\n",
        "    random_state=RANDOM_STATE,\n",
        "    thread_count=N_THREADS,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "model2.fit(X_train_encoded, y_train_custom_split)\n",
        "time_p2 = time.time() - start_time\n",
        "\n",
        "val_pred_p2 = model2.predict(X_val_encoded)\n",
        "rmse_p2 = np.sqrt(mean_squared_error(y_val_custom_split, val_pred_p2))\n",
        "mae_p2 = mean_absolute_error(y_val_custom_split, val_pred_p2)\n",
        "r2_p2 = r2_score(y_val_custom_split, val_pred_p2)\n",
        "\n",
        "print(f\"Время обучения: {time_p2:.2f} секунд\")\n",
        "print(f\"RMSE: {rmse_p2:.4f}\")\n",
        "print(f\"MAE: {mae_p2:.4f}\")\n",
        "print(f\"R2 Score: {r2_p2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline 3: Ensemble (LightGBM + CatBoost)\n",
        "# AutoML принцип: Ансамблирование моделей\n",
        "print(\"=== Pipeline 3: Простой ансамбль (LightGBM + CatBoost) ===\")\n",
        "print(\"Принцип AutoML: Комбинирование нескольких моделей для улучшения предсказаний\")\n",
        "\n",
        "# Use the same preprocessing as Pipeline 2\n",
        "# Используем только числовые признаки (исключаем datetime)\n",
        "X_train_encoded_num = X_train_encoded.select_dtypes(include=[np.number])\n",
        "X_val_encoded_num = X_val_encoded.select_dtypes(include=[np.number])\n",
        "\n",
        "model_lgb = LGBMRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    num_leaves=31,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=N_THREADS,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "model_cat = CatBoostRegressor(\n",
        "    iterations=300,\n",
        "    learning_rate=0.05,\n",
        "    depth=7,\n",
        "    random_state=RANDOM_STATE + 1,\n",
        "    thread_count=N_THREADS,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "model_lgb.fit(X_train_encoded_num, y_train_custom_split)\n",
        "model_cat.fit(X_train_encoded_num, y_train_custom_split)\n",
        "time_p3 = time.time() - start_time\n",
        "\n",
        "val_pred_lgb = model_lgb.predict(X_val_encoded_num)\n",
        "val_pred_cat = model_cat.predict(X_val_encoded_num)\n",
        "val_pred_p3 = (val_pred_lgb + val_pred_cat) / 2  # Simple average\n",
        "\n",
        "rmse_p3 = np.sqrt(mean_squared_error(y_val_custom_split, val_pred_p3))\n",
        "mae_p3 = mean_absolute_error(y_val_custom_split, val_pred_p3)\n",
        "r2_p3 = r2_score(y_val_custom_split, val_pred_p3)\n",
        "\n",
        "print(f\"Время обучения: {time_p3:.2f} секунд\")\n",
        "print(f\"RMSE: {rmse_p3:.4f}\")\n",
        "print(f\"MAE: {mae_p3:.4f}\")\n",
        "print(f\"R2 Score: {r2_p3:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.4. Сравнение всех моделей (AutoML принцип: Model Selection)\n",
        "# Сравним все созданные пайплайны и автоматически выберем лучший\n",
        "\n",
        "print(\"=== Сравнение всех моделей ===\")\n",
        "if 'all_results' in locals():\n",
        "    print(all_results.sort_values('RMSE'))\n",
        "    \n",
        "    # Find best model\n",
        "    best_model_idx = all_results['RMSE'].idxmin()\n",
        "    best_model_name = all_results.loc[best_model_idx, 'Pipeline']\n",
        "    best_rmse_all = all_results.loc[best_model_idx, 'RMSE']\n",
        "    \n",
        "    print(f\"\\nЛучшая модель: {best_model_name}\")\n",
        "    print(f\"Лучший RMSE: {best_rmse_all:.4f}\")\n",
        "else:\n",
        "    print(\"Переменная all_results еще не создана. Запустите ячейку с созданием all_results сначала.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare all pipelines\n",
        "all_results = pd.DataFrame({\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Автоматическая оптимизация гиперпараметров с помощью Optuna\n",
        "try:\n",
        "    import optuna\n",
        "    from sklearn.model_selection import cross_val_score\n",
        "    \n",
        "    print(\"=== Pipeline 4: Автоматическая оптимизация гиперпараметров (Optuna) ===\")\n",
        "    \n",
        "    # Используем только числовые признаки (исключаем datetime)\n",
        "    X_train_encoded_num_p4 = X_train_encoded.select_dtypes(include=[np.number])\n",
        "    X_val_encoded_num_p4 = X_val_encoded.select_dtypes(include=[np.number])\n",
        "    \n",
        "    def objective_lgb(trial):\n",
        "        \"\"\"Функция для оптимизации гиперпараметров LightGBM\"\"\"\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "        }\n",
        "        \n",
        "        model = LGBMRegressor(\n",
        "            **params,\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=N_THREADS,\n",
        "            verbose=-1\n",
        "        )\n",
        "        \n",
        "        scores = cross_val_score(\n",
        "            model, X_train_encoded_num_p4, y_train_custom_split,\n",
        "            cv=3, scoring='neg_mean_squared_error',\n",
        "            n_jobs=N_THREADS\n",
        "        )\n",
        "        return -scores.mean()\n",
        "    \n",
        "    # Оптимизация\n",
        "    study = optuna.create_study(direction='minimize', study_name='lgb_optimization')\n",
        "    study.optimize(objective_lgb, n_trials=20, timeout=240, show_progress_bar=True)\n",
        "    \n",
        "    print(f\"\\nЛучшие гиперпараметры:\")\n",
        "    for key, value in study.best_params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(f\"\\nЛучший RMSE (CV): {np.sqrt(study.best_value):.4f}\")\n",
        "    \n",
        "    # Обучаем модель с лучшими параметрами\n",
        "    best_params = study.best_params.copy()\n",
        "    model_optimized = LGBMRegressor(\n",
        "        **best_params,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=N_THREADS,\n",
        "        verbose=-1\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    model_optimized.fit(X_train_encoded_num_p4, y_train_custom_split)\n",
        "    time_p4 = time.time() - start_time\n",
        "    \n",
        "    val_pred_p4 = model_optimized.predict(X_val_encoded_num_p4)\n",
        "    rmse_p4 = np.sqrt(mean_squared_error(y_val_custom_split, val_pred_p4))\n",
        "    mae_p4 = mean_absolute_error(y_val_custom_split, val_pred_p4)\n",
        "    r2_p4 = r2_score(y_val_custom_split, val_pred_p4)\n",
        "    \n",
        "    print(f\"\\nВремя обучения: {time_p4:.2f} секунд\")\n",
        "    print(f\"RMSE: {rmse_p4:.4f}\")\n",
        "    print(f\"MAE: {mae_p4:.4f}\")\n",
        "    print(f\"R2 Score: {r2_p4:.4f}\")\n",
        "    \n",
        "    optuna_available = True\n",
        "except ImportError:\n",
        "    print(\"Optuna не установлен. Установите: pip install optuna\")\n",
        "    optuna_available = False\n",
        "    rmse_p4 = float('inf')\n",
        "    mae_p4 = float('inf')\n",
        "    r2_p4 = 0\n",
        "    time_p4 = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.6. Автоматическое ансамблирование с оптимизацией весов (AutoML принцип: Ensemble Learning)\n",
        "\n",
        "Оптимизируем веса для ансамбля моделей:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Автоматическое ансамблирование с оптимизацией весов\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "print(\"=== Pipeline 5: Оптимизированный ансамбль ===\")\n",
        "\n",
        "# Собираем предсказания всех моделей на валидации\n",
        "ensemble_models = {\n",
        "    'LGB': model_lgb,\n",
        "    'CatBoost': model_cat,\n",
        "}\n",
        "\n",
        "if optuna_available and 'model_optimized' in locals():\n",
        "    ensemble_models['LGB_Optimized'] = model_optimized\n",
        "\n",
        "# Получаем предсказания (используем только числовые признаки)\n",
        "X_val_encoded_num_p5 = X_val_encoded.select_dtypes(include=[np.number])\n",
        "ensemble_predictions = {}\n",
        "for name, model in ensemble_models.items():\n",
        "    ensemble_predictions[name] = model.predict(X_val_encoded_num_p5)\n",
        "\n",
        "# Функция для оптимизации весов\n",
        "def objective_weights(weights):\n",
        "    \"\"\"Минимизируем RMSE ансамбля\"\"\"\n",
        "    weights = weights / weights.sum()  # Нормализуем веса\n",
        "    pred = np.zeros(len(y_val_custom_split))\n",
        "    for idx, (name, pred_vals) in enumerate(ensemble_predictions.items()):\n",
        "        pred += weights[idx] * pred_vals\n",
        "    return mean_squared_error(y_val_custom_split, pred)\n",
        "\n",
        "# Оптимизация весов\n",
        "n_models = len(ensemble_models)\n",
        "initial_weights = np.ones(n_models) / n_models\n",
        "bounds = [(0, 1)] * n_models\n",
        "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
        "\n",
        "result = minimize(\n",
        "    objective_weights,\n",
        "    initial_weights,\n",
        "    method='SLSQP',\n",
        "    bounds=bounds,\n",
        "    constraints=constraints\n",
        ")\n",
        "\n",
        "optimal_weights = result.x / result.x.sum()\n",
        "print(f\"\\nОптимальные веса для ансамбля:\")\n",
        "for idx, (name, _) in enumerate(ensemble_predictions.items()):\n",
        "    print(f\"  {name}: {optimal_weights[idx]:.4f}\")\n",
        "\n",
        "# Предсказания с оптимальными весами\n",
        "val_pred_ensemble = np.zeros(len(y_val_custom_split))\n",
        "for idx, (name, pred_vals) in enumerate(ensemble_predictions.items()):\n",
        "    val_pred_ensemble += optimal_weights[idx] * pred_vals\n",
        "\n",
        "rmse_p5 = np.sqrt(mean_squared_error(y_val_custom_split, val_pred_ensemble))\n",
        "mae_p5 = mean_absolute_error(y_val_custom_split, val_pred_ensemble)\n",
        "r2_p5 = r2_score(y_val_custom_split, val_pred_ensemble)\n",
        "\n",
        "print(f\"\\nRMSE оптимизированного ансамбля: {rmse_p5:.4f}\")\n",
        "print(f\"MAE: {mae_p5:.4f}\")\n",
        "print(f\"R2 Score: {r2_p5:.4f}\")\n",
        "\n",
        "# Сравнение с простым средним\n",
        "simple_ensemble_pred = np.mean([pred for pred in ensemble_predictions.values()], axis=0)\n",
        "rmse_simple = np.sqrt(mean_squared_error(y_val_custom_split, simple_ensemble_pred))\n",
        "print(f\"\\nRMSE простого среднего: {rmse_simple:.4f}\")\n",
        "print(f\"Улучшение: {rmse_simple - rmse_p5:.4f} ({((rmse_simple - rmse_p5) / rmse_simple * 100):.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all pipelines\n",
        "# Инициализируем переменные для случая, если Optuna не установлен\n",
        "if 'optuna_available' not in locals():\n",
        "    optuna_available = False\n",
        "\n",
        "pipelines_list = [\n",
        "    'LAMA Config 1',\n",
        "    'LAMA Config 2',\n",
        "    'Custom Pipeline 1 (LGB + Auto Feature Selection)',\n",
        "    'Custom Pipeline 2 (CatBoost + Auto Preprocessing)',\n",
        "    'Custom Pipeline 3 (Simple Ensemble)'\n",
        "]\n",
        "rmse_list = [rmse1, rmse2, rmse_p1, rmse_p2, rmse_p3]\n",
        "mae_list = [mae1, mae2, mae_p1, mae_p2, mae_p3]\n",
        "r2_list = [r2_1, r2_2, r2_p1, r2_p2, r2_p3]\n",
        "time_list = [time1, time2, time_p1, time_p2, time_p3]\n",
        "\n",
        "# Добавляем оптимизированные модели, если они доступны\n",
        "if optuna_available and 'rmse_p4' in locals():\n",
        "    pipelines_list.append('Custom Pipeline 4 (Auto Hyperparameter Optimization)')\n",
        "    rmse_list.append(rmse_p4)\n",
        "    mae_list.append(mae_p4)\n",
        "    r2_list.append(r2_p4)\n",
        "    time_list.append(time_p4)\n",
        "\n",
        "if 'rmse_p5' in locals():\n",
        "    pipelines_list.append('Custom Pipeline 5 (Optimized Ensemble)')\n",
        "    rmse_list.append(rmse_p5)\n",
        "    mae_list.append(mae_p5)\n",
        "    r2_list.append(r2_p5)\n",
        "    time_list.append(time_p3 + 10)  # Примерное время для ансамбля\n",
        "\n",
        "all_results = pd.DataFrame({\n",
        "    'Pipeline': pipelines_list,\n",
        "    'RMSE': rmse_list,\n",
        "    'MAE': mae_list,\n",
        "    'R2 Score': r2_list,\n",
        "    'Time (seconds)': time_list\n",
        "})\n",
        "\n",
        "print(\"=== Сравнение всех моделей ===\")\n",
        "print(all_results.sort_values('RMSE'))\n",
        "\n",
        "# Find best model\n",
        "best_model_idx = all_results['RMSE'].idxmin()\n",
        "best_model_name = all_results.loc[best_model_idx, 'Pipeline']\n",
        "best_rmse_all = all_results.loc[best_model_idx, 'RMSE']\n",
        "\n",
        "print(f\"\\nЛучшая модель: {best_model_name}\")\n",
        "print(f\"Лучший RMSE: {best_rmse_all:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions for test set\n",
        "print(\"=== Генерация предсказаний для тестового набора ===\")\n",
        "\n",
        "# Фильтруем только те строки, для которых нужно предсказать рейтинг (rating == NaN)\n",
        "test_df_to_predict = test_df[test_df['rating'].isna()].copy()\n",
        "print(f\"Всего строк в test_df: {len(test_df)}\")\n",
        "print(f\"Строк для предсказания (rating == NaN): {len(test_df_to_predict)}\")\n",
        "\n",
        "# Prepare test data только для строк с NaN rating\n",
        "test_enhanced = create_additional_features(test_df_to_predict, games_df, turns_features)\n",
        "print(f\"Строк после создания признаков: {len(test_enhanced)}\")\n",
        "\n",
        "# Use best custom model for final predictions\n",
        "# Автоматически выбираем лучшую модель (принцип AutoML: автоматический выбор модели)\n",
        "if 'optuna_available' not in locals():\n",
        "    optuna_available = False\n",
        "\n",
        "best_custom_rmse = min([rmse_p1, rmse_p2, rmse_p3])\n",
        "if optuna_available and 'rmse_p4' in locals():\n",
        "    best_custom_rmse = min(best_custom_rmse, rmse_p4)\n",
        "if 'rmse_p5' in locals():\n",
        "    best_custom_rmse = min(best_custom_rmse, rmse_p5)\n",
        "\n",
        "if 'rmse_p5' in locals() and rmse_p5 == best_custom_rmse:\n",
        "    # Use optimized ensemble\n",
        "    X_test = test_enhanced[feature_cols_enhanced].copy()\n",
        "    \n",
        "    for col in categorical_cols_custom:\n",
        "        if col in X_test.columns:\n",
        "            le = label_encoders[col]\n",
        "            test_col = X_test[col].astype(str).fillna('unknown')\n",
        "            known = set(le.classes_)\n",
        "            test_col = test_col.apply(lambda x: x if x in known else 'unknown')\n",
        "            X_test[col] = le.transform(test_col)\n",
        "    \n",
        "    X_test = X_test.fillna(0)\n",
        "    # Используем только числовые признаки (модели обучены на числовых данных)\n",
        "    X_test_num = X_test.select_dtypes(include=[np.number])\n",
        "    \n",
        "    # Используем оптимизированные веса\n",
        "    test_predictions = np.zeros(len(X_test_num))\n",
        "    for idx, (name, model) in enumerate(ensemble_models.items()):\n",
        "        test_predictions += optimal_weights[idx] * model.predict(X_test_num)\n",
        "    \n",
        "    print(\"Использована модель: Оптимизированный ансамбль (AutoML принцип: Ensemble Learning)\")\n",
        "elif optuna_available and 'rmse_p4' in locals() and rmse_p4 == best_custom_rmse:\n",
        "    # Use optimized model\n",
        "    X_test = test_enhanced[feature_cols_enhanced].copy()\n",
        "    \n",
        "    for col in categorical_cols_custom:\n",
        "        if col in X_test.columns:\n",
        "            le = label_encoders[col]\n",
        "            test_col = X_test[col].astype(str).fillna('unknown')\n",
        "            known = set(le.classes_)\n",
        "            test_col = test_col.apply(lambda x: x if x in known else 'unknown')\n",
        "            X_test[col] = le.transform(test_col)\n",
        "    \n",
        "    X_test = X_test.fillna(0)\n",
        "    # Используем только числовые признаки (модель обучена на числовых данных)\n",
        "    X_test_num = X_test.select_dtypes(include=[np.number])\n",
        "    test_predictions = model_optimized.predict(X_test_num)\n",
        "    \n",
        "    print(\"Использована модель: Оптимизированный LightGBM (AutoML принцип: Hyperparameter Optimization)\")\n",
        "elif rmse_p3 < rmse_p2 and rmse_p3 < rmse_p1:\n",
        "    # Use ensemble\n",
        "    X_test = test_enhanced[feature_cols_enhanced].copy()\n",
        "    \n",
        "    # Apply same preprocessing\n",
        "    for col in categorical_cols_custom:\n",
        "        if col in X_test.columns:\n",
        "            le = label_encoders[col]\n",
        "            test_col = X_test[col].astype(str).fillna('unknown')\n",
        "            known = set(le.classes_)\n",
        "            test_col = test_col.apply(lambda x: x if x in known else 'unknown')\n",
        "            X_test[col] = le.transform(test_col)\n",
        "    \n",
        "    X_test = X_test.fillna(0)\n",
        "    # Используем только числовые признаки (модели обучены на числовых данных)\n",
        "    X_test_num = X_test.select_dtypes(include=[np.number])\n",
        "    \n",
        "    test_pred_lgb = model_lgb.predict(X_test_num)\n",
        "    test_pred_cat = model_cat.predict(X_test_num)\n",
        "    test_predictions = (test_pred_lgb + test_pred_cat) / 2\n",
        "    \n",
        "    print(\"Использована модель: Ensemble (LightGBM + CatBoost)\")\n",
        "elif rmse_p2 < rmse_p1:\n",
        "    # Use CatBoost\n",
        "    X_test = test_enhanced[feature_cols_enhanced].copy()\n",
        "    \n",
        "    for col in categorical_cols_custom:\n",
        "        if col in X_test.columns:\n",
        "            le = label_encoders[col]\n",
        "            test_col = X_test[col].astype(str).fillna('unknown')\n",
        "            known = set(le.classes_)\n",
        "            test_col = test_col.apply(lambda x: x if x in known else 'unknown')\n",
        "            X_test[col] = le.transform(test_col)\n",
        "    \n",
        "    X_test = X_test.fillna(0)\n",
        "    # Используем только числовые признаки (для консистентности)\n",
        "    X_test_num = X_test.select_dtypes(include=[np.number])\n",
        "    test_predictions = model2.predict(X_test_num)\n",
        "    \n",
        "    print(\"Использована модель: CatBoost\")\n",
        "else:\n",
        "    # Use LightGBM\n",
        "    X_test = test_enhanced[feature_cols_enhanced].copy()\n",
        "    \n",
        "    # Обработка категориальных признаков для Pipeline 1\n",
        "    for col in categorical_cols_p1:\n",
        "        if col in X_test.columns:\n",
        "            le = label_encoders_p1[col]\n",
        "            test_col = X_test[col].astype(str).fillna('unknown')\n",
        "            known = set(le.classes_)\n",
        "            test_col = test_col.apply(lambda x: x if x in known else 'unknown')\n",
        "            X_test[col] = le.transform(test_col)\n",
        "    \n",
        "    X_test = X_test.fillna(0)\n",
        "    # Используем только числовые признаки (как в Pipeline 1)\n",
        "    X_test_num = X_test.select_dtypes(include=[np.number])\n",
        "    X_test_selected = selector.transform(X_test_num)\n",
        "    test_predictions = model1.predict(X_test_selected)\n",
        "    \n",
        "    print(\"Использована модель: LightGBM\")\n",
        "\n",
        "# Create submission file\n",
        "# Создаём DataFrame с предсказаниями\n",
        "predictions_df = pd.DataFrame({\n",
        "    'game_id': test_enhanced['game_id'].values,\n",
        "    'nickname': test_enhanced['nickname'].values,\n",
        "    'rating': test_predictions\n",
        "})\n",
        "\n",
        "# В sample_submission.csv только game_id, поэтому для каждого game_id берём средний рейтинг\n",
        "# или рейтинг первого игрока (если несколько игроков на игру)\n",
        "submission_ratings = predictions_df.groupby('game_id')['rating'].first().reset_index()\n",
        "\n",
        "# Объединяем с sample_submission_df для правильного порядка\n",
        "submission = sample_submission_df[['game_id']].merge(\n",
        "    submission_ratings,\n",
        "    on='game_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Проверяем, что все рейтинги заполнены\n",
        "if submission['rating'].isna().sum() > 0:\n",
        "    print(f\"⚠️  ВНИМАНИЕ: {submission['rating'].isna().sum()} строк без предсказаний!\")\n",
        "    # Заполняем пропуски средним значением\n",
        "    submission['rating'] = submission['rating'].fillna(submission['rating'].mean())\n",
        "    print(f\"Пропуски заполнены средним значением: {submission['rating'].mean():.2f}\")\n",
        "\n",
        "# Save submission\n",
        "submission_path = RESULTS_DIR / 'submission.csv'\n",
        "submission.to_csv(submission_path, index=False)\n",
        "print(f\"\\nФайл submission сохранен: {submission_path}\")\n",
        "print(f\"\\nПример предсказаний:\")\n",
        "print(submission.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='7'></a>\n",
        "## 7. Выводы и сравнение результатов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# RMSE comparison\n",
        "all_results_sorted = all_results.sort_values('RMSE')\n",
        "axes[0].barh(all_results_sorted['Pipeline'], all_results_sorted['RMSE'])\n",
        "axes[0].set_title('Сравнение RMSE по моделям', fontsize=14)\n",
        "axes[0].set_xlabel('RMSE')\n",
        "\n",
        "# R2 Score comparison\n",
        "axes[1].barh(all_results_sorted['Pipeline'], all_results_sorted['R2 Score'])\n",
        "axes[1].set_title('Сравнение R2 Score по моделям', fontsize=14)\n",
        "axes[1].set_xlabel('R2 Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Текстовые выводы и анализ результатов\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"АНАЛИЗ РЕЗУЛЬТАТОВ И ВЫВОДЫ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Сортируем результаты по RMSE\n",
        "all_results_sorted = all_results.sort_values('RMSE')\n",
        "best_model = all_results_sorted.iloc[0]\n",
        "worst_model = all_results_sorted.iloc[-1]\n",
        "\n",
        "print(f\"\\n📊 ОБЩАЯ СТАТИСТИКА:\")\n",
        "print(f\"   Всего протестировано моделей: {len(all_results)}\")\n",
        "print(f\"   Диапазон RMSE: {all_results['RMSE'].min():.4f} - {all_results['RMSE'].max():.4f}\")\n",
        "print(f\"   Средний RMSE: {all_results['RMSE'].mean():.4f}\")\n",
        "print(f\"   Средний R2 Score: {all_results['R2 Score'].mean():.4f}\")\n",
        "\n",
        "print(f\"\\n🏆 ЛУЧШАЯ МОДЕЛЬ:\")\n",
        "print(f\"   Название: {best_model['Pipeline']}\")\n",
        "print(f\"   RMSE: {best_model['RMSE']:.4f}\")\n",
        "print(f\"   MAE: {best_model['MAE']:.4f}\")\n",
        "print(f\"   R2 Score: {best_model['R2 Score']:.4f}\")\n",
        "print(f\"   Время обучения: {best_model['Time (seconds)']:.2f} секунд\")\n",
        "\n",
        "# Анализ почему эта модель лучше\n",
        "print(f\"\\n💡 ПОЧЕМУ ЭТА МОДЕЛЬ ЛУЧШЕ:\")\n",
        "if 'Ensemble' in best_model['Pipeline'] or 'ансамбль' in best_model['Pipeline']:\n",
        "    print(\"   • Ансамблирование моделей позволяет комбинировать сильные стороны разных алгоритмов\")\n",
        "    print(\"   • Разные модели могут улавливать различные паттерны в данных\")\n",
        "    print(\"   • Оптимизация весов ансамбля улучшает общую точность предсказаний\")\n",
        "    print(\"   • Снижает риск переобучения за счет усреднения предсказаний\")\n",
        "    print(\"   • Более стабильные результаты на новых данных\")\n",
        "elif 'Optimized' in best_model['Pipeline'] or 'Optuna' in best_model['Pipeline']:\n",
        "    print(\"   • Автоматическая оптимизация гиперпараметров находит оптимальные настройки\")\n",
        "    print(\"   • Поиск по пространству параметров позволяет улучшить качество модели\")\n",
        "    print(\"   • Кросс-валидация обеспечивает надежную оценку производительности\")\n",
        "    print(\"   • Балансирует сложность модели и обобщающую способность\")\n",
        "    print(\"   • Учитывает специфику данных через адаптацию параметров\")\n",
        "elif 'CatBoost' in best_model['Pipeline']:\n",
        "    print(\"   • CatBoost эффективно обрабатывает категориальные признаки\")\n",
        "    print(\"   • Встроенная обработка пропусков и категорий упрощает предобработку\")\n",
        "    print(\"   • Хорошо работает с неструктурированными данными\")\n",
        "    print(\"   • Устойчив к переобучению благодаря регуляризации\")\n",
        "    print(\"   • Автоматически обрабатывает категориальные переменные без ручного кодирования\")\n",
        "elif 'LightGBM' in best_model['Pipeline']:\n",
        "    print(\"   • LightGBM эффективен для больших объемов данных\")\n",
        "    print(\"   • Автоматический отбор признаков улучшает качество модели\")\n",
        "    print(\"   • Быстрое обучение и хорошая точность\")\n",
        "    print(\"   • Эффективное использование памяти\")\n",
        "    print(\"   • Хорошо работает с различными типами признаков\")\n",
        "elif 'LAMA' in best_model['Pipeline']:\n",
        "    print(\"   • LightAutoML автоматически выполняет предобработку данных\")\n",
        "    print(\"   • Встроенное ансамблирование нескольких алгоритмов\")\n",
        "    print(\"   • Автоматический выбор признаков и их трансформация\")\n",
        "    print(\"   • Оптимизация гиперпараметров из коробки\")\n",
        "    print(\"   • Минимум ручной настройки при хорошем качестве\")\n",
        "\n",
        "# Сравнение с худшей моделью\n",
        "improvement = ((worst_model['RMSE'] - best_model['RMSE']) / worst_model['RMSE']) * 100\n",
        "print(f\"\\n📈 СРАВНЕНИЕ С ХУДШЕЙ МОДЕЛЬЮ:\")\n",
        "print(f\"   Худшая модель: {worst_model['Pipeline']} (RMSE: {worst_model['RMSE']:.4f})\")\n",
        "print(f\"   Улучшение: {improvement:.2f}% ({worst_model['RMSE'] - best_model['RMSE']:.4f} единиц RMSE)\")\n",
        "\n",
        "# Сравнение со второй лучшей моделью\n",
        "if len(all_results_sorted) > 1:\n",
        "    second_best = all_results_sorted.iloc[1]\n",
        "    improvement_vs_second = ((second_best['RMSE'] - best_model['RMSE']) / second_best['RMSE']) * 100\n",
        "    print(f\"\\n📊 СРАВНЕНИЕ СО ВТОРОЙ ЛУЧШЕЙ МОДЕЛЬЮ:\")\n",
        "    print(f\"   Вторая лучшая: {second_best['Pipeline']} (RMSE: {second_best['RMSE']:.4f})\")\n",
        "    print(f\"   Преимущество лучшей модели: {improvement_vs_second:.2f}% ({second_best['RMSE'] - best_model['RMSE']:.4f} единиц RMSE)\")\n",
        "    if improvement_vs_second < 1:\n",
        "        print(f\"   ⚠ Разница минимальна - обе модели показывают схожее качество\")\n",
        "    elif improvement_vs_second < 5:\n",
        "        print(f\"   ✓ Заметное улучшение - лучшая модель имеет преимущество\")\n",
        "    else:\n",
        "        print(f\"   ✓ Значительное улучшение - лучшая модель существенно превосходит\")\n",
        "\n",
        "# Анализ по категориям моделей\n",
        "print(f\"\\n📋 АНАЛИЗ ПО КАТЕГОРИЯМ:\")\n",
        "lama_models = all_results[all_results['Pipeline'].str.contains('LAMA', case=False)]\n",
        "custom_models = all_results[~all_results['Pipeline'].str.contains('LAMA', case=False)]\n",
        "\n",
        "if len(lama_models) > 0:\n",
        "    print(f\"\\n   LightAutoML модели:\")\n",
        "    print(f\"   • Средний RMSE: {lama_models['RMSE'].mean():.4f}\")\n",
        "    print(f\"   • Лучший RMSE: {lama_models['RMSE'].min():.4f}\")\n",
        "    print(f\"   • Преимущества: автоматическая предобработка, выбор признаков, ансамблирование\")\n",
        "\n",
        "if len(custom_models) > 0:\n",
        "    print(f\"\\n   Собственные модели:\")\n",
        "    print(f\"   • Средний RMSE: {custom_models['RMSE'].mean():.4f}\")\n",
        "    print(f\"   • Лучший RMSE: {custom_models['RMSE'].min():.4f}\")\n",
        "    print(f\"   • Преимущества: контроль над процессом, возможность тонкой настройки\")\n",
        "\n",
        "# Детальный анализ метрик\n",
        "print(f\"\\n📉 ДЕТАЛЬНЫЙ АНАЛИЗ МЕТРИК:\")\n",
        "print(f\"   RMSE (Root Mean Squared Error): {best_model['RMSE']:.4f}\")\n",
        "print(f\"     → Средняя ошибка предсказания: ±{best_model['RMSE']:.2f} единиц рейтинга\")\n",
        "print(f\"   MAE (Mean Absolute Error): {best_model['MAE']:.4f}\")\n",
        "print(f\"     → Средняя абсолютная ошибка: {best_model['MAE']:.2f} единиц рейтинга\")\n",
        "print(f\"   R² Score: {best_model['R2 Score']:.4f}\")\n",
        "if best_model['R2 Score'] > 0.9:\n",
        "    print(f\"     → Модель объясняет более 90% дисперсии - отличное качество!\")\n",
        "elif best_model['R2 Score'] > 0.8:\n",
        "    print(f\"     → Модель объясняет более 80% дисперсии - очень хорошее качество\")\n",
        "elif best_model['R2 Score'] > 0.6:\n",
        "    print(f\"     → Модель объясняет более 60% дисперсии - хорошее качество\")\n",
        "elif best_model['R2 Score'] > 0.4:\n",
        "    print(f\"     → Модель объясняет более 40% дисперсии - приемлемое качество\")\n",
        "else:\n",
        "    print(f\"     → Модель объясняет менее 40% дисперсии - требуется улучшение\")\n",
        "\n",
        "# Рекомендации\n",
        "print(f\"\\n💼 РЕКОМЕНДАЦИИ ДЛЯ ПРОДАКШЕНА:\")\n",
        "if best_model['R2 Score'] > 0.8:\n",
        "    print(\"   ✓ Модель показывает отличное качество (R2 > 0.8)\")\n",
        "    print(\"   ✓ Готова к использованию в продакшене\")\n",
        "elif best_model['R2 Score'] > 0.6:\n",
        "    print(\"   ✓ Модель показывает хорошее качество (R2 > 0.6)\")\n",
        "    print(\"   ✓ Можно использовать в продакшене с мониторингом\")\n",
        "else:\n",
        "    print(\"   ⚠ Модель можно улучшить (R2 < 0.6)\")\n",
        "    print(\"   ⚠ Рекомендуется дополнительная работа над признаками и моделью\")\n",
        "\n",
        "if best_model['Time (seconds)'] < 300:\n",
        "    print(f\"   ✓ Время обучения приемлемое ({best_model['Time (seconds)']:.2f} сек < 5 минут)\")\n",
        "    print(\"   ✓ Модель можно переобучать регулярно\")\n",
        "else:\n",
        "    print(f\"   ⚠ Время обучения можно оптимизировать ({best_model['Time (seconds)']:.2f} сек)\")\n",
        "    print(\"   ⚠ Рекомендуется использовать для стабильных данных или уменьшить сложность\")\n",
        "\n",
        "print(f\"\\n   🎯 ИТОГОВАЯ РЕКОМЕНДАЦИЯ:\")\n",
        "print(f\"   Для продакшена рекомендуется использовать: {best_model['Pipeline']}\")\n",
        "print(f\"   Эта модель обеспечивает баланс между:\")\n",
        "print(f\"     • Точностью: RMSE = {best_model['RMSE']:.4f}, R² = {best_model['R2 Score']:.4f}\")\n",
        "print(f\"     • Скоростью обучения: {best_model['Time (seconds)']:.2f} секунд\")\n",
        "print(f\"     • Стабильностью: MAE = {best_model['MAE']:.4f}\")\n",
        "\n",
        "# Выводы о принципах AutoML\n",
        "print(f\"\\n🤖 ПРИМЕНЕНИЕ ПРИНЦИПОВ AUTOML:\")\n",
        "if 'Ensemble' in best_model['Pipeline'] or 'ансамбль' in best_model['Pipeline']:\n",
        "    print(\"   ✓ Принцип ансамблирования оказался наиболее эффективным\")\n",
        "    print(\"   ✓ Комбинация нескольких моделей дала лучший результат\")\n",
        "elif 'Optimized' in best_model['Pipeline'] or 'Optuna' in best_model['Pipeline']:\n",
        "    print(\"   ✓ Принцип автоматической оптимизации гиперпараметров показал эффективность\")\n",
        "    print(\"   ✓ Систематический поиск оптимальных параметров улучшил качество\")\n",
        "elif 'LAMA' in best_model['Pipeline']:\n",
        "    print(\"   ✓ Полноценный AutoML фреймворк показал хорошие результаты\")\n",
        "    print(\"   ✓ Автоматизация всех этапов ML pipeline эффективна\")\n",
        "else:\n",
        "    print(\"   ✓ Автоматическая предобработка и выбор признаков улучшили качество\")\n",
        "    print(\"   ✓ Применение принципов AutoML дало конкурентные результаты\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Заключение\n",
        "\n",
        "В данном проекте были:\n",
        "\n",
        "1. **Проведен анализ целевой переменной**: изучено распределение рейтинга, выявлены аномальные значения, проведен временной анализ.\n",
        "\n",
        "2. **Проведен анализ признаков**: классифицированы признаки по типам, проанализированы пропущенные значения, изучены зависимости между признаками, определена важность признаков.\n",
        "\n",
        "3. **Создан бейзлайн с LightAutoML**: протестированы 2 различные конфигурации, выбрана лучшая.\n",
        "\n",
        "4. **Реализовано собственное решение**: созданы 3 различных пайплайна с разными подходами к предобработке и моделированию.\n",
        "\n",
        "5. **Проведено сравнение результатов**: все модели сравнены по метрикам RMSE, MAE и R2 Score.\n",
        "\n",
        "Лучшая модель будет использована для генерации финальных предсказаний на тестовом наборе данных.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
